version: '3.8'

services:
  ollama:
    image: ollama/ollama:0.3.6
    container_name: ollama
    restart: always
    networks:
      - llm_network
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_KEEP_ALIVE=-1
    volumes:
      - ollama:/root/.ollama
      - ${MODEL_SAVE_PATH}:/local_models
    ${GPU_OPTION}

  text-embeddings-inference:
    image: ${text-embeddings-inference-image}
    container_name: text-embeddings-inference
    restart: always
    networks:
      - llm_network
    entrypoint: bash
    command: -c "/tei_start.sh"
    volumes:
      - tei:/data
      - ${WORK_DIR}/tei_start.sh:/tei_start.sh
    ${TEI_GPU_OPTION}

  db:
    image: postgres
    container_name: db
    restart: always
    networks:
      - llm_network
    volumes:
      - db_data:/var/lib/postgresql/data
    environment:
      - POSTGRES_PASSWORD=example
    healthcheck:
      test: ["CMD", "pg_isready"]
      interval: 1s
      timeout: 5s
      retries: 10

  adminer:
    image: michalhosna/adminer
    container_name: adminer
    restart: always
    networks:
      - llm_network
    ports:
      - "8888:8080"
    environment:
      - ADMINER_DRIVER=pgsql
      - ADMINER_AUTOLOGIN=0
      - ADMINER_USERNAME=postgres
      - ADMINER_PASSWORD=example
      - ADMINER_SERVER=db
      - ADMINER_DB=postgres
      - ADMINER_NAME=Litellm

  litellm:
    image: ghcr.io/berriai/litellm:main-v1.43.7
    container_name: litellm
    restart: always
    networks:
      - llm_network
    ports:
      - "4000:4000"
    environment:
      - DATABASE_URL=postgresql://postgres:example@db:5432/postgres
      - STORE_MODEL_IN_DB=True
      - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY}

  anythingllm:
    image: mintplexlabs/anythingllm
    container_name: anythingllm
    restart: always
    networks:
      - llm_network
    ports:
      - "3001:3001"
    cap_add:
      - SYS_ADMIN
    volumes:
      - ${STORAGE_LOCATION}:/app/server/storage
      - ${STORAGE_LOCATION}/.env:/app/server/.env
    environment:
      - STORAGE_DIR=/app/server/storage

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    restart: always
    networks:
      - llm_network
    ports:
      - "3000:8080"
    environment:
      - OPENAI_API_BASE_URL=http://litellm:4000
    volumes:
      - open-webui:/app/backend/data

  # qdrant
  qdrant:
    image: qdrant/qdrant
    container_name: qdrant
    restart: always
    ports:
      - "6333:6333"
    volumes:
      - /home/ubuntu/storage_Qdrant:/qdrant/storage
    networks:
      - llm_network 


  nginx:
    image: nginx
    container_name: nginx
    restart: always
    networks:
      - llm_network
    ports:
      - "80:80"
    volumes:
      - ${NGINX_HTML_LOCATION}:/usr/share/nginx/html
      - ${NGINX_HTML_LOCATION}:/etc/nginx/conf.d


networks:
  llm_network:

volumes:
  ollama:
  db_data:
  open-webui:
  tei:
  qdrant:
